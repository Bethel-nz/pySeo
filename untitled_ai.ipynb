{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tce3stUlHN0L"
      },
      "source": [
        "##### Copyright 2024 Google LLC.\n",
        "\n",
        "all copy right reserved for google, without them, this wouldn't be possible"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9__zr1nSBpE"
      },
      "source": [
        "# Before you start:\n",
        "* Get Your Key from [https://aistudio.google.com/app/apikey](https://aistudio.google.com/app/apikey)\n",
        "* User Guide: How to Use the SEO Bot\n",
        "* click on the key icon by your left\n",
        "* add new secret and paste it there\n",
        "\n",
        "\n",
        "\n",
        "**Prerequisites**\n",
        "\n",
        "* **A website URL:** Have the URL of the website you'd like to analyze ready.\n",
        "* **Python Environment:** You can clone this notebook or run it directly here on [https://colab.research.google.com/](https://colab.research.google.com/)\n",
        "\n",
        "**Steps**\n",
        "\n",
        "1. **Run the Code:**\n",
        "   * Open the Python script for this SEO bot in a code editor or your preferred Python environment.\n",
        "   * Locate the \"play\" button or equivalent function to execute the code (this will vary depending on your environment).\n",
        "\n",
        "2. **Initialization:** Wait for the bot to initialize. You might see some output indicating that it's loading necessary libraries.\n",
        "\n",
        "3. **Input Website URL:** You'll be prompted to enter the website URL you want to analyze. Type in the URL and press Enter.\n",
        "\n",
        "4. **Analysis and Tip Generation:** The bot will:\n",
        "   * Crawl your website.\n",
        "   * Analyze its SEO elements.\n",
        "   * Utilize Gemini (your AI model) to generate optimization tips.\n",
        "   * You may see progress updates during this process.\n",
        "\n",
        "5. **Save Results:**\n",
        "   * Once the analysis is complete, a file named \"seo_analysis.txt\" will be created.\n",
        "   * Locate this file (usually in the same directory as your code) and open it to view the detailed SEO analysis and tips.\n",
        "\n",
        "**Example Usage**\n",
        "\n",
        "Let's say you want to analyze the website \"[www.example.com](https://www.example.com)\". You would follow these steps, inputting \"[www.example.com](https://www.example.com)\" when prompted.\n",
        "\n",
        "**Additional Notes**\n",
        "\n",
        "* **Analysis Time:** The time it takes to analyze a website depends on its size and complexity.\n",
        "* **Customization:** If you're familiar with Python, you can customize the SEO bot's behavior by modifying the code.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "NPmqeWak7RRW"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFNV1e3ASJha"
      },
      "source": [
        "### Install the SDK's\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9OEoeosRTv-5",
        "outputId": "bba7e34e-900c-4108-fe1a-346bf22fb945",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: summarizer in /usr/local/lib/python3.10/dist-packages (0.0.7)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from summarizer) (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->summarizer) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->summarizer) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->summarizer) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->summarizer) (4.66.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.10/dist-packages (0.17.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.2.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.38.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n",
            "Requirement already satisfied: textstat in /usr/local/lib/python3.10/dist-packages (0.7.3)\n",
            "Requirement already satisfied: pyphen in /usr/local/lib/python3.10/dist-packages (from textstat) (0.14.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install -q -U google-generativeai\n",
        "!pip install summarizer\n",
        "!pip install nltk textblob\n",
        "!pip install requests\n",
        "!pip install beautifulsoup4\n",
        "!pip install transformers\n",
        "!pip install textstat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCFF5VSTbcAR"
      },
      "source": [
        "### Import packages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRC2HngneEeQ"
      },
      "source": [
        "Import the necessary packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TS9l5igubpHO"
      },
      "outputs": [],
      "source": [
        "import pathlib\n",
        "import textwrap\n",
        "\n",
        "import google.generativeai as genai\n",
        "\n",
        "from IPython.display import display\n",
        "from IPython.display import Markdown\n",
        "from transformers import pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d10c38a5c91f"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "!python -m nltk.downloader all\n"
      ],
      "metadata": {
        "id": "pvWIWeRZ1B4-",
        "outputId": "e24aa646-cf44-434a-8969-8db4214e7c15",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/lib/python3.10/runpy.py:126: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
            "  warn(RuntimeWarning(msg))\n",
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Package abc is already up-to-date!\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Package alpino is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
            "[nltk_data]    |       up-to-date!\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package bcp47 to /root/nltk_data...\n",
            "[nltk_data]    |   Package bcp47 is already up-to-date!\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Package brown is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Package chat80 is already up-to-date!\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package city_database is already up-to-date!\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    |   Package comtrans is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Package crubadan is already up-to-date!\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Package dolch is already up-to-date!\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package extended_omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Package floresta is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Package ieer is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Package indian is already up-to-date!\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    |   Package jeita is already up-to-date!\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Package kimmo is already up-to-date!\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    |   Package knbc is already up-to-date!\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    |   Package machado is already up-to-date!\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Package paradigms is already up-to-date!\n",
            "[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n",
            "[nltk_data]    |   Package pe08 is already up-to-date!\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Package pil is already up-to-date!\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Package pl196x is already up-to-date!\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Package porter_test is already up-to-date!\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Package ppattach is already up-to-date!\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    |   Package propbank is already up-to-date!\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Package ptb is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Package qc is already up-to-date!\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    |   Package reuters is already up-to-date!\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Package rslp is already up-to-date!\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Package rte is already up-to-date!\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    |   Package semcor is already up-to-date!\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Package senseval is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Package smultron is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Package state_union is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Package swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Package switchboard is already up-to-date!\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Package tagsets is already up-to-date!\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Package timit is already up-to-date!\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Package toolbox is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package verbnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Package webtext is already up-to-date!\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet2022 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet2022 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Package ycoe is already up-to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHYFrFPjSGNq"
      },
      "source": [
        "### Running the bot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ab9ASynfcIZn",
        "outputId": "fc7ad289-ab8d-4af0-d17e-ce0002d30cc3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the website URL: https://alivexem.vercel.app/\n",
            " skipped 1 times\n",
            " skipped 2 times\n",
            " skipped 3 times\n",
            " skipped 4 times\n",
            " skipped 5 times\n",
            " skipped 6 times\n",
            " skipped 7 times\n",
            " skipped 9 times\n",
            " skipped 10 times\n",
            " skipped 11 times\n",
            " skipped 12 times\n",
            " skipped 13 times\n",
            " skipped 14 times\n",
            " skipped 15 times\n",
            "Analyzed 1 subpages so far...\n",
            "SEO analysis saved to 'seo_analysis.txt'\n",
            "SEO analysis saved to seo_analysis.txt\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import textstat\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import word_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "from textblob import TextBlob\n",
        "import os.path\n",
        "from urllib.parse import urlparse, urljoin\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "\n",
        "# you can add to this list\n",
        "SOCIAL_DOMAINS = [\n",
        "    \"facebook.com\",\n",
        "    \"instagram.com\",\n",
        "    \"github.com\",\n",
        "    \"youtube.com\",\n",
        "    \"telegram.org\",\n",
        "    \"twitter.com\",\n",
        "    \"linkedin.com\",\n",
        "    \"githubstatus.com\",\n",
        "    \"vercel.app\"\n",
        "]\n",
        "IGNORED_EXTENSIONS = [\n",
        "    \".pdf\", \".doc\", \".docx\", \".xlsx\", \".ppt\", \".pptx\",  # Add more as needed\n",
        "]\n",
        "\n",
        "# *** Helper Functions ***\n",
        "def crawl_page(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "    return soup\n",
        "\n",
        "def extract_subpages(soup, base_domain, base_url):\n",
        "    subpages = []\n",
        "    skipped_count = 0  # Counter for skipped links\n",
        "\n",
        "    for link in soup.find_all('a', href=True):\n",
        "        subpage_url = link['href']\n",
        "\n",
        "        # Handle relative URLs\n",
        "        if not urlparse(subpage_url).netloc:\n",
        "            subpage_url = urljoin(base_url, subpage_url)\n",
        "\n",
        "        # Check for ignored websites and keywords\n",
        "        subpage_domain = urlparse(subpage_url).netloc\n",
        "        if any(domain in subpage_domain for domain in SOCIAL_DOMAINS):\n",
        "            skipped_count += 1\n",
        "            continue  # Skip to the next link\n",
        "\n",
        "        # Subdomain Check (Existing Logic)\n",
        "        subpage_parts = subpage_domain.split('.')\n",
        "        if len(subpage_parts) > 2:\n",
        "            if subpage_parts[-2:] != base_domain.split('.'):\n",
        "                skipped_count += 1\n",
        "                print(f\" skipped {skipped_count } times\")\n",
        "                continue\n",
        "\n",
        "        subpages.append(subpage_url)\n",
        "\n",
        "    return subpages\n",
        "\n",
        "\n",
        "\n",
        "def extract_seo_elements(soup):\n",
        "    img_tags = soup.find_all('img')\n",
        "    header_tags = [tag for tag in soup.find_all() if tag.name in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']]\n",
        "    p_tags = soup.find_all('p')\n",
        "    meta_tags = {}  # Create a dictionary for meta tags\n",
        "    for tag in soup.find_all('meta'):\n",
        "        if 'name' in tag.attrs:\n",
        "            meta_tags[tag['name']] = tag.get('content', '')  # Store content (if it exists)\n",
        "\n",
        "    return img_tags, header_tags, p_tags, meta_tags\n",
        "\n",
        "\n",
        "def extract_text_from_tags(img_tags, header_tags, p_tags):\n",
        "    text_content = \"\"\n",
        "    for tag in img_tags:\n",
        "        if 'alt' in tag.attrs:\n",
        "            text_content += tag['alt'] + \" \"\n",
        "    for tag in header_tags:\n",
        "        text_content += tag.text.strip() + \" \"\n",
        "    for tag in p_tags:\n",
        "        text_content += tag.text.strip() + \" \"\n",
        "    return text_content\n",
        "\n",
        "def generate_website_summary(text_content):\n",
        "\n",
        "    summary = summarizer(text_content, max_length=250, min_length=150)\n",
        "\n",
        "    return summary[0]['summary_text']\n",
        "\n",
        "\n",
        "def build_prompt(summary, img_tags, header_tags, meta_tags):\n",
        "    prompt = f\"\"\"You are an SEO expert and a staff software engineer that understands how seo is good for marketing. I will provide you with:\n",
        "\n",
        "* **Website Summary:** {summary}\n",
        "* **SEO Elements:**\n",
        "    * **Image Tags:** {img_tags}\n",
        "    * **Headers:** {header_tags}\n",
        "    * **Meta Tags:** {meta_tags}\n",
        "\n",
        "Analyze this website. Provide specific SEO tips with examples for the following:\n",
        "\n",
        "* **Image Optimization:** How can images be improved for SEO? **For example:**\n",
        "    * Adding descriptive alt text to images (e.g., <img alt=\"Golden Retriever puppy playing fetch\" src=\"puppy.jpg\"/>).\n",
        "    * Optimizing image file names to include relevant keywords (e.g., \"golden-retriever-puppy-playing.jpg\").\n",
        "* **Header Structure:** How can the headers be better organized to improve SEO?  **For example:**\n",
        "    * Using a clear hierarchy (H1, H2, H3, etc.), with the most important keywords in the H1 tag.\n",
        "    * Keeping headers concise and informative.\n",
        "* **Content:** Are there areas for improvement with keywords or readability? **For example:**\n",
        "    * Naturally integrating relevant keywords throughout the text, especially in headings.\n",
        "    * Using short paragraphs and bullet points to improve readability.\n",
        "* **Meta Tags:** Are they optimized and present? Can they be improved?  **For example:**\n",
        "    * Ensuring the meta title tag accurately describes the page content.\n",
        "    * Writing a compelling meta description that summarizes the page and entices users to click.\n",
        "\n",
        "**Please provide clear instructions and actionable examples and they can be as much as many points, that you know would be a great feat for a webpage to be ranked .**\n",
        "\"\"\"\n",
        "    return prompt\n",
        "\n",
        "\n",
        "def calculate_seo_score(seo_elements, text_content):\n",
        "    score = 0\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    # Keyword analysis\n",
        "    content_words = word_tokenize(text_content.lower())\n",
        "    filtered_words = [w for w in content_words if w not in stop_words and w.isalpha()]\n",
        "    fdist = FreqDist(filtered_words)\n",
        "\n",
        "    # Determine the most frequent keyword\n",
        "    most_common_keyword = fdist.most_common(1)[0][0]\n",
        "    keywords = [most_common_keyword]\n",
        "\n",
        "    keyword_density = sum(w in keywords for w in filtered_words) / len(filtered_words)\n",
        "    if keyword_density > 0.02:\n",
        "        score += 15\n",
        "\n",
        "    # Image optimization check\n",
        "    if any(\"alt\" in tag.attrs and tag['alt'] for tag in seo_elements['img_tags']):\n",
        "        score += 10\n",
        "\n",
        "    # Meta tags check\n",
        "    if 'title' in seo_elements['meta_tags'] and \\\n",
        "        'description' in seo_elements['meta_tags'] and \\\n",
        "        'author' in seo_elements['meta_tags']:\n",
        "        score += 10\n",
        "\n",
        "    # Readability\n",
        "    readability = textstat.flesch_reading_ease(text_content)\n",
        "    if readability > 60:\n",
        "        score += 5\n",
        "\n",
        "    return score\n",
        "\n",
        "# *** Main Analysis Function ***\n",
        "def analyze_website(url):\n",
        "    base_domain = urlparse(url).netloc\n",
        "    results = {\n",
        "        \"pages\": {} # To organize results per page\n",
        "    }\n",
        "    visited_urls = set()\n",
        "    urls_to_visit = [url]\n",
        "    total_subpages = 0  # Counter for tracking analyzed pages\n",
        "\n",
        "    while urls_to_visit:\n",
        "        current_url = urls_to_visit.pop(0)\n",
        "        if current_url in visited_urls:\n",
        "            continue\n",
        "\n",
        "        visited_urls.add(current_url)\n",
        "        results['pages'][current_url] = {}  # Initialize the page's data\n",
        "        error_file_name = \"seo_analysis_error.txt\"\n",
        "        results_file_name = \"seo_analysis.txt\"\n",
        "        try:\n",
        "            soup = crawl_page(current_url)\n",
        "\n",
        "            # SEO Analysis\n",
        "            img_tags, header_tags, p_tags, meta_tags = extract_seo_elements(soup)\n",
        "            text_content = extract_text_from_tags(img_tags, header_tags, p_tags)\n",
        "            website_summary = generate_website_summary(text_content)\n",
        "            prompt = build_prompt(website_summary, img_tags, header_tags, meta_tags)\n",
        "\n",
        "            # SEO Score Calculation\n",
        "            seo_score = calculate_seo_score(\n",
        "                {\"img_tags\": img_tags, \"header_tags\": header_tags, \"meta_tags\": meta_tags},\n",
        "                text_content\n",
        "            )\n",
        "            results['pages'][current_url]['seo_score'] = seo_score\n",
        "\n",
        "            # Gemini Interaction\n",
        "            model = genai.GenerativeModel('gemini-pro')\n",
        "            response = model.generate_content(prompt, stream=True)\n",
        "            seo_tips = \"\"\n",
        "            for chunk in response:\n",
        "                seo_tips += chunk.text\n",
        "\n",
        "            results['pages'][current_url]['seo_tips'] = seo_tips\n",
        "            results['pages'][current_url]['img_tags'] = img_tags  # Store for clarity\n",
        "            results['pages'][current_url]['header_tags'] = header_tags\n",
        "            results['pages'][current_url]['p_tags'] = p_tags\n",
        "            results['pages'][current_url]['meta_tags'] = meta_tags\n",
        "\n",
        "            # Discover subpages\n",
        "            soup = crawl_page(current_url)\n",
        "            subpages = extract_subpages(soup, base_domain, current_url)  # Call the function\n",
        "            urls_to_visit.extend(subpages)\n",
        "\n",
        "\n",
        "            total_subpages += 1\n",
        "            print(f\"Analyzed {total_subpages} subpages so far...\")\n",
        "\n",
        "\n",
        "        except Exception as e:\n",
        "            with open(error_file_name, \"a\") as error_file:\n",
        "                error_file.write(f\"Error analyzing {current_url}\\n Error: {e}\\n\")\n",
        "            print(f\"Error analyzing {current_url}. Details written to '{error_file_name}'\")\n",
        "\n",
        "        else:  # Execute if no exception occurred\n",
        "            with open(results_file_name, \"w\") as f:\n",
        "              for url, data in results['pages'].items():  # Iterate over pages\n",
        "                f.write(f\"URL: {url}\\n\")\n",
        "                f.write(f\"SEO Tips: {data.get('seo_tips', 'Analysis not available')}\\n\")\n",
        "                f.write(\"-\"*20 + \"\\n\")\n",
        "\n",
        "            print(f\"SEO analysis saved to '{results_file_name}'\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# Get website URL as input\n",
        "my_website_url = input(\"Enter the website URL: \")\n",
        "\n",
        "# Analyze and save results\n",
        "results = analyze_website(my_website_url)\n",
        "\n",
        "\n",
        "print(\"SEO analysis saved to seo_analysis.txt\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "google": {
      "image_path": "/static/site-assets/images/docs/logo-python.svg",
      "keywords": [
        "examples",
        "gemini",
        "beginner",
        "googleai",
        "quickstart",
        "python",
        "text",
        "chat",
        "vision",
        "embed"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}